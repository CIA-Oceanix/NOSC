# @package _global_

multivar:
  ssh:
    var_path: /Odyssey/private/t22picar/data/train_glorys_15_8th/glorys_multivar_2017_2018.nc
    #mask_path: /Odyssey/public/glorys/obs_masks/global_obs_6sats_masks_2022.pickle
    var_name: zos
    input_arch: prior_input
    output_arch: no_output
    broadcast_time: False
  sst:
    var_path: /Odyssey/private/t22picar/data/train_glorys_15_8th/glorys_multivar_2017_2018.nc
    #mask_path: /Odyssey/public/glorys/obs_masks/global_obs_6sats_masks_2022.pickle
    var_name: thetao
    input_arch: prior_input
    output_arch: no_output
    broadcast_time: False
  u10:
    var_path: /Odyssey/private/t22picar/data/train_glorys_15_8th/era5_2017_2018.nc
    #mask_path: /Odyssey/public/glorys/obs_masks/global_obs_6sats_masks_2022.pickle
    var_name: u10
    input_arch: prior_input
    output_arch: no_output
    broadcast_time: False
  v10:
    var_path: /Odyssey/private/t22picar/data/train_glorys_15_8th/era5_2017_2018.nc
    #mask_path: /Odyssey/public/glorys/obs_masks/global_obs_6sats_masks_2022.pickle
    var_name: v10
    input_arch: prior_input
    output_arch: no_output
    broadcast_time: False
  uo:
    var_path: /Odyssey/private/t22picar/data/train_glorys_15_8th/glorys_multivar_2017_2018.nc
    #mask_path: /Odyssey/public/glorys/obs_masks/global_obs_6sats_masks_2022.pickle
    var_name: uo
    input_arch: no_input
    output_arch: full_output
    broadcast_time: False
  vo:
    var_path: /Odyssey/private/t22picar/data/train_glorys_15_8th/glorys_multivar_2017_2018.nc
    #mask_path: /Odyssey/public/glorys/obs_masks/global_obs_6sats_masks_2022.pickle
    var_name: vo
    input_arch: no_input
    output_arch: full_output
    broadcast_time: False

multivar_selector:
  _target_: contrib.multivar.multivar_utils.MultivarBatchSelector

domain:
  train:
    lat:
      _target_: builtins.slice
      _args_:
      - -80
      - 90
    lon:
      _target_: builtins.slice
      _args_:
      - -180
      - 180
  test:
    lat:
      _target_: builtins.slice
      _args_:
      - -79
      - 89
    lon:
      _target_: builtins.slice
      _args_:
      - -179
      - 179

trainer:
  _target_: pytorch_lightning.Trainer
  inference_mode: False
  gradient_clip_val: 0.5
  accelerator: gpu
  devices: 1
  logger:
    _target_: pytorch_lightning.loggers.CSVLogger
    save_dir: ${hydra:runtime.output_dir}
    name: ${hydra:runtime.choices.xp}
    version: ''
  max_epochs: 350
  limit_train_batches: 102
  callbacks:
    - _target_: src.versioning_cb.VersioningCallback
    - _target_: pytorch_lightning.callbacks.LearningRateMonitor
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      monitor: val_total_mse
      save_top_k: 3
      filename: '{val_total_mse:.5f}-{epoch:03d}'

datamodule:
  _target_: contrib.multivar.multivar_data.MultivarDataModule
  multivar_da: 
    _target_: contrib.data_loading.data.open_multivar_datasets
    vars_info: ${multivar}
    domain: ${domain.train}
    full_time_domain: ${datamodule.domains}
  domains:
    train:
      time: {_target_: builtins.slice, _args_: ['2017-01-01', '2017-12-31']}
    val: 
      time: {_target_: builtins.slice, _args_: ['2018-01-01', '2018-12-31']}
    test: 
      time: {_target_: builtins.slice,  _args_: ['2018-01-01', '2018-12-31']}
  xrds_kw:
    patch_dims: { time: 11, lat: 1440, lon: 2880}
    strides: { time: 1, lat: 1432, lon: 2872}
    domain_limits: ${domain.train}
  dl_kw: {batch_size: 1, num_workers: 8}

model:
  _target_: contrib.multivar.multivar_models_unet.MultivarUNet
  multivar_selector: ${multivar_selector}
  persist_rw: False
  opt_fn:
    _target_: contrib.multivar.multivar_models_unet.cosanneal_lr_adam_unet
    _partial_: true
    lr: 1e-3
    T_max: ${trainer.max_epochs}
  rec_weight:
      _target_: contrib.multivar.multivar_utils.get_multivar_mapping_wei
      patch_dims: ${datamodule.xrds_kw.patch_dims}
      crop: {time: 0, lat: 4, lon: 4}
      dims_out:
        _target_: contrib.multivar.multivar_utils.get_multivar_prior_dims_out
        multivar_dict: ${multivar}
        channels_per_dim: ${datamodule.xrds_kw.patch_dims.time}
      offset: 1

  #rec_weight_fn:
  #  _target_: contrib.forecast_weights.forecast_weights_reconstruction_soft_edges
  #  _partial_: True
  #  soft_edge: 0.25
  solver:
    _target_: contrib.multivar.multivar_models_unet.UNet #MultivarGradSolverZero
    #multivar_selector: ${multivar_selector}
    #n_step: 0
    #lr_grad: 1e3
    n_channels: 
      _target_: contrib.multivar.multivar_models_unet.get_multivar_only_prior_dims_in
      multivar_dict: ${multivar}
      channels_per_dim: ${datamodule.xrds_kw.patch_dims.time}
    n_classes: 
      _target_: contrib.multivar.multivar_utils.get_multivar_prior_dims_out
      multivar_dict: ${multivar}
      channels_per_dim: ${datamodule.xrds_kw.patch_dims.time}
    #grad_mod:
    #  _target_: contrib.multivar.multivar_models_unet.AdamOpti
    #  parameters: contrib.multivar.multivar_models_unet.UNet.parameters
    #  dim_in: 
    #    _target_: contrib.multivar.multivar_utils.get_multivar_grad_dims
    #    multivar_dict: ${multivar}
    #    channels_per_dim: ${datamodule.xrds_kw.patch_dims.time}
    #  dim_hidden: 96
    #prior_cost:
    #  _target_: contrib.multivar.multivar_models.MultivarBilinAEPriorCost
    #  multivar_selector: ${multivar_selector}
    #  dim_in: 
    #    _target_: contrib.multivar.multivar_utils.get_multivar_prior_dims_in
    #    multivar_dict: ${multivar}
    #    channels_per_dim: ${datamodule.xrds_kw.patch_dims.time}
    #  dim_out: 
    #    _target_: contrib.multivar.multivar_utils.get_multivar_prior_dims_out
    #    multivar_dict: ${multivar}
    #    channels_per_dim: ${datamodule.xrds_kw.patch_dims.time}
    #  dim_hidden: 64
    #  bilin_quad: False
      # bilin_quad: True
      #downsamp: 2
    #obs_cost:
    #  _target_: contrib.multivar.multivar_models.MultivarBaseObsCost
    #  multivar_selector: ${multivar_selector}

  pre_metric_fn:
        _target_: xarray.Dataset.sel
        _partial_: True
        time: {_target_: builtins.slice, _args_: ["2021-03-14", "2021-04-20"]}
        lat: ${domain.test.lat}
        lon: ${domain.test.lon}

entrypoints:
  - _target_: pytorch_lightning.seed_everything
    seed: 333
  - _target_: src.train.base_training
    trainer: ${trainer}
    lit_mod: ${model}
    dm: ${datamodule}

defaults:
  - _self_

